{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\IDEA\\OverFeat\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchsummaryX import summary\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2+cu113\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") # device 정의\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3080'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverFeat_accurate(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # train with 221x221 5 random crops and their horizontal filps\n",
    "        # mini- batches of size 128\n",
    "        # initialized weight randomly with mu=0, sigma=1x10^-2\n",
    "        # SGD, momentum=0.6, l2 weight decay of 1x10^-5\n",
    "        # learning rate 5x10^-2, decay by 0.5 after (30, 50, 60, 70, 80) epochs\n",
    "        # Dropout on FCN?? -> dropout before classifier conv layer\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # no contrast normalization is used\n",
    "            # max polling with non-overlapping\n",
    "            # 1st and 2nd layer stride 2 instead of 4\n",
    "\n",
    "            # 1st\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=7, stride=2),  # (b x 96 x 108 x 108)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3),  # (b x 96 x 36 x 36)\n",
    "\n",
    "            # 2nd\n",
    "            nn.Conv2d(96, 256, 7, stride= 1),  # (b x 256 x 30 x 30)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (b x 256 x 15 x 15)\n",
    "\n",
    "            # 3rd\n",
    "            nn.Conv2d(256, 512, 3, padding=1),  # (b x 512 x 15 x 15)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 4th\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # (b x 512 x 15 x 15)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 5th\n",
    "            nn.Conv2d(512, 1024, 3, padding=1),  # (b x 1024 x 15 x 15)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 6th\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1),  # (b x 1024 x 15 x 15)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3),  # (b x 1024 x 5 x 5)\n",
    "        )\n",
    "\n",
    "        # fully connecyed layers implemented as a convolution layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 7th\n",
    "            nn.Dropout(p=0.5, inplace=True),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=4096, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 8th\n",
    "            nn.Dropout(p=0.5, inplace=True),\n",
    "            nn.Conv2d(4096, 4096, 1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 9th\n",
    "            nn.Conv2d(4096, num_classes, 1)\n",
    "        )\n",
    "\n",
    "        self.init_weight()  # initialize weight\n",
    "\n",
    "    def init_weight(self):\n",
    "        for layer in self.feature_extractor:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the net.\n",
    "        Args:\n",
    "            x (Tensor): input tensor\n",
    "        Returns:\n",
    "            output (Tensor): output tensor\n",
    "        \"\"\"\n",
    "        x = self.feature_extractor(x)\n",
    "        return self.classifier(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverFeat_fast(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # train with 221x221 5 random crops and their horizontal filps\n",
    "        # mini- batches of size 128\n",
    "        # initialized weight randomly with mu=0, sigma=1x10^-2\n",
    "        # SGD, momentum=0.6, l2 weight decay of 1x10^-5\n",
    "        # learning rate 5x10^-2, decay by 0.5 after (30, 50, 60, 70, 80) epochs\n",
    "        # Dropout on FCN?? -> dropout before classifier conv layer\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # no contrast normalization is used\n",
    "            # max polling with non-overlapping\n",
    "            # 1st and 2nd layer stride 2 instead of 4\n",
    "\n",
    "            # 1st\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  # (b x 96 x 56 x 56)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (b x 96 x 28 x 28)\n",
    "\n",
    "            # 2nd\n",
    "            nn.Conv2d(96, 256, 5, stride= 1),  # (b x 256 x 24 x 24)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (b x 256 x 12 x 12)\n",
    "\n",
    "            # 3rd\n",
    "            nn.Conv2d(256, 512, 3, padding=1),  # (b x 512 x 12 x 12)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 4th\n",
    "            nn.Conv2d(512, 1024, 3, padding=1),  # (b x 1024 x 12 x 12)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 5th\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1),  # (b x 1024 x 12 x 12)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (b x 1024 x 6 x 6)\n",
    "        )\n",
    "\n",
    "        # fully connecyed layers implemented as a convolution layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 6th\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=3072, kernel_size=6),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 7th\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Conv2d(3072, 4096, 1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 8th\n",
    "            nn.Conv2d(4096, num_classes, 1)\n",
    "        )\n",
    "\n",
    "        self.init_weight()  # initialize weight\n",
    "\n",
    "    def init_weight(self):\n",
    "        for layer in self.feature_extractor:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the net.\n",
    "        Args:\n",
    "            x (Tensor): input tensor\n",
    "        Returns:\n",
    "            output (Tensor): output tensor\n",
    "        \"\"\"\n",
    "        x = self.feature_extractor(x)\n",
    "        return self.classifier(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img_train = \"../ILSVRC2012/ILSVRC2012_img_train/\"\n",
    "path_img_val = \"../ILSVRC2012/ILSVRC2012_img_val/\"\n",
    "\n",
    "path_log = \"/tblog/\"\n",
    "path_checkpoint = \"/checkpoints/\"\n",
    "if not(os.path.exists(path_log)):\n",
    "    os.makedirs(path_log)\n",
    "if not(os.path.exists(path_checkpoint)):\n",
    "    os.makedirs(path_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used seed : 18760258861600\n"
     ]
    }
   ],
   "source": [
    "seed = torch.initial_seed()\n",
    "print(f'Used seed : {seed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbwriter = SummaryWriter(log_dir=path_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fast = True\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================================\n",
      "                                         Kernel Shape         Output Shape  \\\n",
      "Layer                                                                        \n",
      "0_feature_extractor.Conv2d_0          [3, 96, 11, 11]    [128, 96, 56, 56]   \n",
      "1_feature_extractor.ReLU_1                          -    [128, 96, 56, 56]   \n",
      "2_feature_extractor.MaxPool2d_2                     -    [128, 96, 28, 28]   \n",
      "3_feature_extractor.Conv2d_3          [96, 256, 5, 5]   [128, 256, 24, 24]   \n",
      "4_feature_extractor.ReLU_4                          -   [128, 256, 24, 24]   \n",
      "5_feature_extractor.MaxPool2d_5                     -   [128, 256, 12, 12]   \n",
      "6_feature_extractor.Conv2d_6         [256, 512, 3, 3]   [128, 512, 12, 12]   \n",
      "7_feature_extractor.ReLU_7                          -   [128, 512, 12, 12]   \n",
      "8_feature_extractor.Conv2d_8        [512, 1024, 3, 3]  [128, 1024, 12, 12]   \n",
      "9_feature_extractor.ReLU_9                          -  [128, 1024, 12, 12]   \n",
      "10_feature_extractor.Conv2d_10     [1024, 1024, 3, 3]  [128, 1024, 12, 12]   \n",
      "11_feature_extractor.ReLU_11                        -  [128, 1024, 12, 12]   \n",
      "12_feature_extractor.MaxPool2d_12                   -    [128, 1024, 6, 6]   \n",
      "13_classifier.Dropout_0                             -    [128, 1024, 6, 6]   \n",
      "14_classifier.Conv2d_1             [1024, 3072, 6, 6]    [128, 3072, 1, 1]   \n",
      "15_classifier.ReLU_2                                -    [128, 3072, 1, 1]   \n",
      "16_classifier.Dropout_3                             -    [128, 3072, 1, 1]   \n",
      "17_classifier.Conv2d_4             [3072, 4096, 1, 1]    [128, 4096, 1, 1]   \n",
      "18_classifier.ReLU_5                                -    [128, 4096, 1, 1]   \n",
      "19_classifier.Conv2d_6             [4096, 1000, 1, 1]    [128, 1000, 1, 1]   \n",
      "\n",
      "                                       Params     Mult-Adds  \n",
      "Layer                                                        \n",
      "0_feature_extractor.Conv2d_0          34.944k   109.283328M  \n",
      "1_feature_extractor.ReLU_1                  -             -  \n",
      "2_feature_extractor.MaxPool2d_2             -             -  \n",
      "3_feature_extractor.Conv2d_3         614.656k     353.8944M  \n",
      "4_feature_extractor.ReLU_4                  -             -  \n",
      "5_feature_extractor.MaxPool2d_5             -             -  \n",
      "6_feature_extractor.Conv2d_6         1.18016M   169.869312M  \n",
      "7_feature_extractor.ReLU_7                  -             -  \n",
      "8_feature_extractor.Conv2d_8        4.719616M   679.477248M  \n",
      "9_feature_extractor.ReLU_9                  -             -  \n",
      "10_feature_extractor.Conv2d_10      9.438208M  1.358954496G  \n",
      "11_feature_extractor.ReLU_11                -             -  \n",
      "12_feature_extractor.MaxPool2d_12           -             -  \n",
      "13_classifier.Dropout_0                     -             -  \n",
      "14_classifier.Conv2d_1             113.24928M   113.246208M  \n",
      "15_classifier.ReLU_2                        -             -  \n",
      "16_classifier.Dropout_3                     -             -  \n",
      "17_classifier.Conv2d_4             12.587008M    12.582912M  \n",
      "18_classifier.ReLU_5                        -             -  \n",
      "19_classifier.Conv2d_6                 4.097M        4.096M  \n",
      "-----------------------------------------------------------------------------------------------------\n",
      "                            Totals\n",
      "Total params           145.920872M\n",
      "Trainable params       145.920872M\n",
      "Non-trainable params           0.0\n",
      "Mult-Adds             2.801403904G\n",
      "=====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\IDEA_torch\\lib\\site-packages\\torchsummaryX\\torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sum = df.sum()\n"
     ]
    }
   ],
   "source": [
    "if(Fast):\n",
    "    overfeat = OverFeat_fast(num_classes=1000).to(device)\n",
    "    summary(overfeat, torch.zeros((128,3,231,231),device=device))\n",
    "else:\n",
    "    overfeat = OverFeat_accurate(num_classes=1000).to(device)\n",
    "    summary(overfeat, torch.zeros((128,3,221,221),device=device))\n",
    "\n",
    "# overfeat = torch.nn.parallel.DataParallel(overfeat, device_ids=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Fast):\n",
    "    cropsize = 231\n",
    "else:\n",
    "    cropsize = 221\n",
    "\n",
    "transfrom = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(cropsize),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datasets.ImageFolder(path_img_train, transform=transfrom)\n",
    "dataset_val = datasets.ImageFolder(path_img_val, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    dataset=dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(\n",
    "    params=overfeat.parameters(),\n",
    "    momentum=0.6,\n",
    "    weight_decay=1e-5,\n",
    "    lr=5e-2\n",
    ")\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[30, 50, 60, 70, 80], gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfeat.train()\n",
    "step = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for imgs, classes in dataloader_train:\n",
    "        imgs, classes = imgs.to(device), classes.to(device)\n",
    "\n",
    "        output = overfeat(imgs)\n",
    "        loss = F.cross_entropy(output, classes)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if(step % 1e3 == 0):\n",
    "            with torch.no_grad():\n",
    "                _, preds = torch.max(output, 1)\n",
    "                accuracy = torch.sum(preds == classes)\n",
    "\n",
    "                print(f'Epoch: {epoch + 1} \\tStep: {step} \\tLoss: {loss.item():.4f} \\tAcc: {accuracy.item()}')\n",
    "                tbwriter.add_scalar('loss', loss.item(), step)\n",
    "                tbwriter.add_scalar('accuracy', accuracy.item(), step)\n",
    "\n",
    "                for name, parameter in overfeat.named_parameters():\n",
    "                    if parameter.grad is not None:\n",
    "                        avg_grad = torch.mean(parameter.grad)\n",
    "                        print(f'\\t{name} - grad_avg: {avg_grad}')\n",
    "                        tbwriter.add_scalar(f'grad_avg/{name}', avg_grad.item(), step)\n",
    "                        tbwriter.add_histogram(f'grad/{name}', parameter.grad.cpu().numpy(), step)\n",
    "                    if parameter.data is not None:\n",
    "                        avg_weight = torch.mean(parameter.data)\n",
    "                        print(f'\\t{name} - param_avg: {avg_weight}')\n",
    "                        tbwriter.add_histogram(f'weight/{name}', parameter.data.cpu().numpy(), step)\n",
    "                        tbwriter.add_scalar(f'weight_avg/{name}', avg_weight.item(), step)\n",
    "\n",
    "                overfeat.eval()\n",
    "                for val_imgs, val_classes in dataloader_val:\n",
    "                    val_imgs, val_classes = val_imgs.to(device), val_classes.to(device)\n",
    "\n",
    "                    val_output = overfeat(val_imgs)\n",
    "                    val_loss = F.cross_entropy(val_output, val_classes)\n",
    "                    \n",
    "                    _, val_preds = torch.max(val_output, 1)\n",
    "                    val_accuracy = torch.sum(val_preds == val_classes)\n",
    "\n",
    "                    print(f'\\tValidation Loss: {val_loss} \\t Validation Acc: {val_accuracy.item()}')\n",
    "                    tbwriter.add_scalar('val_loss', val_loss.item(), step)\n",
    "                    tbwriter.add_scalar('val_accuracy', val_accuracy.item(), step)\n",
    "                overfeat.train()\n",
    "\n",
    "        step += 1\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    if(Fast):\n",
    "        checkpoint_path = os.path.join(path_checkpoint, f'overfeat_fast_states_epoch{epoch}.pkl')\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(path_checkpoint, f'overfeat_accurate_states_epoch{epoch}.pkl')\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': overfeat.state_dict(),\n",
    "        'seed' : seed\n",
    "    }\n",
    "    torch.save(state, checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 torch",
   "language": "python",
   "name": "idea_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
